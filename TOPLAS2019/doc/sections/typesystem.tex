\section{Gradually Typed Implicit Polymorphism}
\label{sec:type-system}

In \cref{sec:exploration} we introduced our consistent
subtyping relation that accommodates polymorphic types. In
this section we continue with the development by giving a declarative
type system for predicative implicit polymorphism that employs the consistent
subtyping relation. The declarative system itself is already quite
interesting as it is equipped with both higher-rank polymorphism and
the unknown type.
% Moreover, unlike non-gradual type systems with
% higher-rank polymorphism, guessed types affect runtime behaviour if
% used by the implicit casts, which raises concerns with respect to
% coherency. Our response to those concerns is given in \cref{subsec:algo:discuss},
% after we give a simple
% algorithm that implements the declarative system
% (\cref{sec:algorithm}) and discuss soundness and completeness.
The syntax of expressions in the declarative system is given below:
\begin{center}
  \begin{tabular}{lrcl} \toprule
    Expressions &  $[[e]]$   & $\Coloneqq$ & $[[x]]  \mid [[n]]  \mid [[\x : A . e]] \mid [[ \x . e ]] \mid [[e1 e2]] \mid [[ let x = e1 in e2  ]]$ \\ \bottomrule
  \end{tabular}
\end{center}
Meta-variable $[[e]]$ ranges over expressions.
Expressions include variables $[[x]]$, integers $[[n]]$, annotated lambda
abstractions $[[\x : A . e]]$, un-annotated lambda abstractions $[[\x . e]]$,
applications $[[e1 e2]]$, and let expressions $[[let x = e1 in e2]]$.

% We write $A$, $B$ for
% types. Types are either the integer type $\nat$, type variables $a$, functions
% types $A \to B$, universal quantification $\forall a. A$, or the unknown type
% $\unknown$. Though we only have one base type $\nat$, we also use $\bool$ for
% the purpose of illustration. Monotypes $\tau$ contain all types other than the
% universal quantifier and the unknown type. Contexts $\dctx$ is an \emph{ordered} list
% that record types for term variables,
% and all type variables with the expected well-formedness
% condition.

\subsection{Typing in Detail}

\Cref{fig:decl-typing} gives the typing rules for our declarative system
(the reader is advised to ignore the gray-shaded parts for now). \Rref{var}
extracts the type of the variable from the typing context. \Rref{int} always
infers integer types. \Rref{lamann} puts $x$ with type annotation $A$ into
the context, and continues type checking the body $e$. \Rref{lam} assigns a
monotype $\tau$ to $x$, and continues type checking the body $e$. Gradual types
and polymorphic types are introduced via explicit annotations.
\Rref{gen} puts a fresh type variable $a$ into the type context and generalizes
the typing result $A$ to $[[ \/a. A ]]$.
\Rref{let} infers the type $[[A]]$ of $[[e1]]$, then puts $[[x]] : [[A]]$ in the
context to infer the type of $[[e2]]$.
\Rref{app} first
infers the type of $e_1$, then the matching judgment $[[ dd |- A |> A1 -> A2 ]]$ extracts the domain type $A_1$ and the codomain type $A_2$ from type
$A$. The type $A_3$ of the argument $e_2$ is then compared with $A_1$ using the
consistent subtyping judgment.

% \renewcommand{\trto}[1]{\hlmath{\rightsquigarrow{#1}}}
\begin{figure}[t]
  \begin{small}

\drules[]{$ [[dd |- e : A ~~> pe]] $}{Typing}{var, int, gen, lamann, lam, let, app}

\drules[m]{$ [[dd |- A |> A1 -> A2]] $}{Matching}{forall, arr, unknown}


  % \begin{mathpar}
  %   \framebox{$\tpreinf e : A \trto s$} \\
  %   \DVar \and \DNat \and \DGen
  %   \and \DLamAnnA
  %   \and \DLam
  %   \and \DApp
  % \end{mathpar}
  % \begin{mathpar}
  %   \framebox{$\tprematch A \match A_1 \to A_2$} \\
  %   \MMC \\ \MMA \and \MMB
  % \end{mathpar}

  \end{small}
  \caption{Declarative typing}
  \label{fig:decl-typing}
\end{figure}

\paragraph{Matching.} The matching judgment of
\citet{siek2015refined} is extended to polymorphic types naturally,
resulting in $[[ dd|- A |> A1 -> A2   ]]$. In \rref{m-forall}, a monotype
$\tau$ is guessed to instantiate the universal quantifier $a$. This rule is
inspired by the \emph{application judgment} $\Phi \vdash A \bullet e \Rightarrow C$~\citep{dunfield2013complete},
which says that if we apply a term
of type $A$ to an argument $e$, then we get a term of type $C$. If $A$ is a
polymorphic type, the judgment works by guessing instantiations
until it reaches an arrow type. Matching further simplifies the
application judgment, since it is independent of typing. \Rref{m-arr,m-unknown}
are the same as \citet{siek2015refined}. \Rref{m-arr} returns the domain type $A_1$
and range type $A_2$ as expected. If the input is $\unknown$, then
\rref{m-unknown} returns $\unknown$ as both the type for the domain and the range.

Note that matching saves us from having a subsumption rule (\rref{u-sub} in
\cref{fig:original-typing}).
% which is included in the original Odersky-L{\"a}ufer type system.
The subsumption rule is incompatible with consistent subtyping, since the latter
is not transitive. A discussion of a subsumption rule based on normal subtyping
can be found in \cref{subsec:variant}.
% applying the subsumption rule twice, once to $\unknown$, and once to any type we want.


\renewcommand{\trto}[1]{\rightsquigarrow{#1}}
\subsection{Type-directed Translation}
\label{sec:type:trans}

We give the dynamic semantics of our language by translating it to \pbc\citep{ahmed2011blame}. Below
we show a subset of the terms in \pbc that are used in the translation:
\begin{center}
  \begin{tabular}{lrcl} \toprule
    \pbc Terms &  $[[pe]]$   & $\Coloneqq$ & $[[x]] \mid [[n]] \mid [[\x : A . pe]] \mid [[/\a . pe]] \mid [[pe1 pe2]] \mid [[ < A `-> B > pe  ]] $ \\ \bottomrule
  \end{tabular}
\end{center}
A cast $[[ < A `-> B > pe  ]]$ converts the value of term $s$ from type $A$ to type $B$.
A cast from $A$ to $B$ is permitted only if the types are \emph{compatible},
written $A \pbccons B$, as briefly mentioned in
\cref{subsec:consistency-subtyping}. The syntax of types in \pbc is the
same as ours.

The translation is given in the gray-shaded parts in \cref{fig:decl-typing}. The
only interesting case here is to insert explicit casts in the application rule.
Note that there is no need to translate matching or consistent subtyping.
Instead we insert the source and target types of a cast directly in the
translated expressions, thanks to the following two lemmas:

\begin{clemma}[$\match$ to $\pbccons$]%Compatibility of Matching]
  \label{lemma:comp-match}
  If $[[ dd |- A |> A1 -> A2  ]]$, then $A \pbccons A_1 \to A_2$.
\end{clemma}

\begin{clemma}[$\tconssub$ to $\pbccons$]%[Compatibility of Consistent Subtyping]
  \label{lemma:comp-conssub}
  If $[[  dd |- A <~ B ]]$, then $A \pbccons B$.
\end{clemma}

In order to show the correctness of the translation, we prove that our
translation always produces well-typed expressions in \pbc. By
\cref{lemma:comp-match,lemma:comp-conssub}, we have the following theorem:

\renewcommand{\otthl}[1]{#1}

\begin{ctheorem}[Type Safety]
  \label{lemma:type-safety}
  If $[[ dd |- e : A ~~> pe   ]]$, then $[[ dd |- pe : A ]]$.
\end{ctheorem}

\paragraph{Parametricity.} An important semantic property of polymorphic types is
\emph{relational parametricity}~\citep{reynolds1983types}. The parametricity
property says that all instances of a polymorphic function should
behave \emph{uniformly}. % In other words, functions cannot inspect into a type
% variable, and act differently for different instances of the type variable.
A classic example is a function with the type $[[ \/a . a -> a ]]$. The
parametricity property guarantees that a value of this type must be either the
identity function (i.e., $[[ \x. x ]]$) or the undefined function (one which
never returns a value). However, with the addition of the unknown type
$\unknown$, careful measures are to be taken to ensure parametricity. Our
translation target \pbc is taken from \citet{ahmed2011blame}, where relational
parametricity is enforced by dynamic sealing~\citep{matthews2008parametric,
  Neis:2009:NP:1596550.1596572}, but there is no rigorous proof. Later,
\citet{amal2017blame} imposed a syntactic restriction on terms of \pbc, where
all type abstractions must have \emph{values} as their body. With this
invariant, they proved that the restricted \pbc satisfies relational
parametricity. It remains to see if our translation process can be adjusted to
target restricted \pbc. One possibility is to impose similar restriction to the
\rref{gen}:
\[
  \inferrule{
    [[dd, a |- e : A ~~> pv]]
  }{
    [[dd |- e : \/ a. A ~~> /\a. pv]]
  }\rname{Gen2}
\]
where we only generate type abstractions if the inner body is a value.
However, the type system with this rule is a weaker calculus, which is not a
conservative extension of the Odersky-L{\"a}ufer type system.

% If so, based on their result, and by \cref{lemma:type-safety}, parametricity is
% preserved in our system.

\paragraph{Ambiguity from Casts.}

The translation does not always produce a unique target expression. This is
because when guessing some monotype $\tau$ in \rref{m-forall,cs-forallL}, we
could have many choices, which inevitably leads to different types. This is
usually not a problem for (non-gradual) System F-like systems~\citep{jones2007practical, dunfield2013complete}
because they adopt a type-erasure semantics~\citep{pierce2002types}. However, in our case, the choice of monotypes may
affect the runtime behaviour of translated programs, since they could appear
inside the explicit casts. For instance, the following example shows two possible
translations for the same source expression $[[ (\x : unknown . f x) ]] : [[unknown -> int]]$,
where the type of $f$ is instantiated to $[[ int -> int ]]$ and $[[ bool -> int ]]$, respectively:
\begin{align*}
  f: \forall a. a \to \nat &\byinf (\blam x \unknown {f ~ x})
                          : \unknown \to \nat \\
                          &\trto (\blam x \unknown (\cast {\forall a. a \to \nat} {\nat \to \nat} f) ~
                          (\hlmath{\cast \unknown \nat} x))
  \\
  f: \forall a. a \to \nat &\byinf (\blam x \unknown {f ~ x})
                          : \unknown \to \nat \\
                          &\trto (\blam x \unknown (\cast {\forall a. a \to \nat} {\bool \to \nat} f) ~
                          (\hlmath{\cast \unknown \bool} x))
\end{align*}
If we apply $[[ \x : unknown . f x  ]]$ to $3$, which is fine
since the function can take any input, the first translation runs smoothly in
\pbc, while the second one will raise a cast error ($\nat$ cannot be cast to
$\bool$). Similarly, if we apply it to $\truee$, then the second succeeds while
the first fails. The culprit lies in the highlighted parts where the 
instantiation of $a$ appears in the explicit cast. More generally, any
choice introduces an explicit cast to that type in the translation, which causes
a runtime cast error if the function is applied to a value whose type does not
match the guessed type. Note that this does not compromise the type safety of
the translated expressions, since cast errors are part of the type safety
guarantees.

The semantic discrepancy is due to the guessing nature of the \emph{declarative}
system. As far as the static semantics is concerned, both $[[ int -> int  ]]$ and
$[[ bool -> int ]]$ are equally acceptable. But this is not the case at runtime.
The astute reader may have found that the \emph{only} appropriate choice is to
instantiate the type of $f$ to $[[ unknown -> int ]]$ in the matching judgment. However, as specified by
\rref{m-forall} in \cref{fig:decl-typing}, we can only instantiate type variables
to monotypes, but $\unknown$ is \emph{not} a monotype! We will get back to
this issue in \cref{sec:advanced-extension}.

\paragraph{Coherence.}

The ambiguity of translation seems to imply that the declarative system is
\emph{incoherent}. A semantics is coherent if distinct typing derivations of
the same typing judgment possess the same meaning~\citep{Reynolds_coherence}. We
argue that the declarative system is \emph{coherent up to cast errors} in the
sense that a well-typed program produces a unique value, or results in a cast
error. In the above example, suppose $f$ is defined as $(\lambda x .~1)$, then
whatever the translation might be, applying $([[ \x : unknown . f x ]])$ to $3$
either results in a cast error, or produces $1$, nothing else.

We defined contextual equivalence \citep{morris1969lambda} to formally
characterize that two open expressions have the same behavior. The definition of
contextual equivalence requires a notion of well-typed expression contexts
$[[C]]$, written $ [[ CC : (dd |- A) ~~> (dd' |- A') ]] $. The definitions of
contexts and context typing are standard and thus omitted. As is common, we
first define contextual approximation. In our setting, we need to
relax the notion of 
contextual approximation of \pbc~\citep{amal2017blame} to also take into
consideration of cast errors. We write $[[dd]] \vdash
\ctxappro{[[pe1]]}{[[pe2]]}{[[A]]}$ to say that $[[pe2]]$ mimics the behaviour
of $[[pe1]]$ at type $[[A]]$ in the sense that whenever a program containing
$[[pe1]]$ reduces to an integer, replacing it with $[[pe2]]$ either reduces to
the same integer, or emits a cast error.
% or if it diverges, replacing it with $[[pe2]]$ either diverges or emits a cast error.
We restrict the program results
to integers to eliminate the role of types in values. If it is not an integer,
it is always possible to embed it into another context that reduces to an
integer. Then we write $[[dd]] \vdash \ctxeq{[[pe1]]}{[[pe2]]}{[[A]]}$ to say
$[[pe1]]$ and $[[pe2]]$ are contextually equivalent, that is, they approximate
each other.

\begin{definition}[Contextual Approximation and Equivalence up to Cast Errors] \leavevmode
  \label{conj:coher}
  \begin{center}
  \begin{tabular}{lll}
$[[dd]] \vdash \ctxappro{[[pe1]]}{[[pe2]]}{[[A]]}$ & $\defeq$ & $[[ dd |- pe1 : A  ]] \land [[dd |- pe2 : A ]] \ \land $ \\
                                                   & & for all $\mathcal{C}.\, [[ CC : (dd |- A) ~~> (empty |- int) ]] \Longrightarrow$ \\
                                                   & &  $\quad \mathcal{C}\{ [[pe1]] \}   \Downarrow [[n]] \Longrightarrow (\mathcal{C} \{ [[ pe2 ]]  \}  \reduce [[n]] \lor \mathcal{C} \{ [[ pe2 ]]  \}  \reduce \blamev) $ \\
    $[[dd]] \vdash \ctxeq{[[pe1]]}{[[pe2]]}{[[A]]}$ & $\defeq$ & $ [[dd]] \vdash \ctxappro{[[pe1]]}{[[pe2]]}{[[A]]} \land [[dd]] \vdash \ctxappro{[[pe2]]}{[[pe1]]}{[[A]]}$
  \end{tabular}
  \end{center}
\end{definition}

Before presenting the formal definition of coherence, first we observe that after erasing types
and casts, all translations of the same expression are exactly the same. This is easy to see by
examining each elaboration rule. We use $[[ \ pe \]]$ to denote an expression in \pbc after erasure.

\begin{restatable}[]{lemma}{lemmaerase} \label{lemma:erase}
  If $[[ dd |- e : A ~~> pe1 ]]$, and $[[ dd |- e : A ~~> pe2 ]]$,
  then $[[ \ pe1 \ ]] \aeq [[ \ pe2 \ ]]$.
\end{restatable}

Second, at runtime, the only role of types and casts is to emit cast errors caused
by type mismatch. Therefore, By \cref{lemma:erase} coherence follows as a corollary:

\begin{restatable}[Coherence up to cast errors]{lemma}{lemmacoherence}
  \label{lemma:coherence:up}
  For any expression $[[e]]$
  such that $[[ dd |- e : A ~~> pe1    ]]$ and $[[ dd |- e : A ~~> pe2    ]]$, we have
  $[[ dd ]] \vdash \ctxeq{[[pe1]]}{[[pe2]]}{[[A]]} $.
\end{restatable}


\subsection{Correctness Criteria}
\label{sec:criteria}

\citet{siek2015refined} present a set of properties, the \emph{refined criteria}, that a well-designed gradual
typing calculus must have. Among all the
criteria, those related to the static aspects of gradual typing are well
summarized by \citet{cimini2016gradualizer}. Here we review those criteria and
adapt them to our notation. We have proved in Coq that our type system satisfies
all these criteria.

\begin{clemma}[Correctness Criteria]\leavevmode
    \begin{itemize}
    \item \textbf{Conservative extension:}
      for all static $[[dd]]$, $[[e]]$, and $[[A]]$,
      \begin{itemize}
      \item if $[[ dd ||- e : A   ]]$,
        then there exists $[[B]]$,
        such that $[[ dd |- e : B  ]]$,
        and $[[  dd |- B <: A  ]]$.
      \item if $[[ dd |- e : A   ]]$,
        then $[[ dd ||- e : A  ]]$
      \end{itemize}
    \item \textbf{Monotonicity w.r.t. precision:}
      for all $[[dd]], [[e]], [[e']], [[A]]$,
      if $[[ dd |- e : A   ]]$,
      and $[[ e' <<= e  ]]$,
      then $[[ dd |- e' : B     ]]$,
      and $[[ B <<= A ]]$ for some B.
    \item \textbf{Type Preservation of cast insertion:}
      for all $[[dd]], [[e]], [[A]]$,
      if $[[ dd |- e : A   ]]$,
      then $[[ dd |- e : A ~~> pe   ]]$,
      and $[[  dd |- pe : A  ]]$ for some $[[pe]]$.
    \item \textbf{Monotonicity of cast insertion:}
      for all $[[dd]], [[e1]], [[e2]], [[pe1]], [[pe2]], [[A]]$,
      if $[[ dd |- e1 : A ~~> pe1   ]]$,
      and $[[ dd |- e2 : A ~~> pe2    ]]$,
      and $[[ e1 <<= e2 ]]$,
      then $[[ dd | dd |- pe1 <<= pe2]]$.
    \end{itemize}
\end{clemma}


The first criterion states that the gradual type system should be a conservative
extension of the original system. In other words, a \emph{static} program
is typeable in the Odersky-L{\"a}ufer type system if and only if it is typeable
in the gradual type system. A static program is one that does not contain any
type $\unknown$\footnote{Note that the term \emph{static} has appeared several
  times with different meanings.}. However since our gradual type system does
not have the subsumption rule, it produces more general types.
% It also
% ensures that ill-typed programs of the original language remain so in the
% gradual type system.


\begin{figure}[t]
\begin{small}
  \begin{mathpar}
    \framebox{$A \lessp B$}{\quad \text{Type precision}} \\
    \LUnknown \and \LNat \and \LArrow \and \LTVar
    \and \LForall
  \end{mathpar}

  \begin{mathpar}
    \framebox{$e_1 \lessp e_2$}{\quad \text{Term precision}} \\
    \LRefl \and \LAbsAnn \and \LApp
  \end{mathpar}

  \begin{mathpar}
    \framebox{$\dctx_1 \ctxsplit \dctx_2 \bylessp e_1 \lesspp e_2$}
    {\quad \text{Term less precision in \pbc}} \\
    \LVar \and \LNatP
    \and \LAbsAnnP
    \and \LTAbsP \and \LAppP
    \and \LCast
  \end{mathpar}
\end{small}
\caption{Less precision}
\label{fig:lessp}
\end{figure}



The second criterion states that if a typeable expression loses some type
information, it remains typeable. This criterion depends on the definition of
the precision relation, written $[[ A <<= B ]]$, which is given in \cref{fig:lessp}.
The relation intuitively captures a notion of types containing more or less
unknown types ($\unknown$). The precision relation over types lifts to programs,
i.e., $[[ e1 <<= e2  ]]$ means that $e_1$ and $e_2$ are the same program except
that $e_1$ has more unknown types.

The first two criteria are fundamental to gradual typing. They explain for
example why these two programs $(\blam x \nat {x + 1})$ and $(\blam x \unknown
{x + 1})$ are typeable, as the former is typeable in the Odersky-L{\"a}ufer type
system and the latter is a less-precise version of it.

The last two criteria relate the compilation to the cast calculus. The third
criterion is essentially the same as \cref{lemma:type-safety}, given that a
target expression should always exist, which can be easily seen from
\cref{fig:decl-typing}. The last criterion ensures that the translation must be
monotonic over the precision relation $\lessp$. \citet{ahmed2011blame} does not
include a formal definition of precision, but an \emph{approximation}
definition and a \emph{simulation relation}. Here we adapt the simulation
relation as the precision, and a subset of it that is used in our system
is given at the bottom of \cref{fig:lessp}.


\paragraph{The Dynamic Gradual Guarantee.}

Besides the static criteria, there is also a criterion concerning the dynamic
semantics, known as \emph{the dynamic gradual guarantee}~\citep{siek2015refined}.

\begin{definition}[Dynamic Gradual Guarantee]
  Suppose $[[ e' <<= e ]]$, and $ [[ empty |- e : A ~~> pe]] $ and
  $ [[empty |- e' : A' ~~> pe' ]] $,
  \begin{itemize}
    \item if $[[pe ==> pv]]$, then $[[pe' ==> pv']]$ and $[[pv' <<= pv]]$. If $[[pe]] \Uparrow $ then $[[pe']] \Uparrow$.
    \item if $[[pe' ==> pv']]$, then $[[pe ==> pv]]$ where $[[pv' <<= pv]]$,
      or $[[ pe ==> blame ]]$. If $[[pe']] \Uparrow$ then $[[pe]] \Uparrow$ or $[[pe ==> blame]]$.
  \end{itemize}
\end{definition}

The first part of the dynamic gradual guarantee says that if a gradually typed program
evaluates to a value, then making type annotations less precise always produces
a program that evaluates to an less precise value. Unfortunately, coherence
up to cast errors in the declarative system breaks the dynamic gradual guarantee. For
instance:
\begin{mathpar}
  (\blam{f}{\forall a. a \to \nat}{\blam{x}{\nat}{f~x}})~(\lambda x .\, 1)~3 \and
  (\blam{f}{\forall a. a \to \nat}{\blam{x}{\unknown}{f~x}})~(\lambda x .\, 1)~3
\end{mathpar}
The left one evaluates to 1, whereas its less precise version (right) will give
a cast error if $a$ is instantiated to $\bool$ for example.
In \cref{sec:advanced-extension}, we will present an extension of the declarative system
that will alleviate the issue.

% As for the dynamic grudual guarantee, things become a bit murky for two reasons: (1) as
% we discussed before, our declarative system is incoherent in that the runtime
% behaviour of the same source program can vary depending on the particular
% translation; (2) it is still unknown whether dynamic grudual guarantee holds in \pbc. We
% will have more discussion on the dynamic grudual guarantee in \cref{subsec:dynamic}.


% As discussed in \cref{subsec:algo:discuss}, we could design a more sophisticated
% declarative/algorithmic type system where coherence is retained. However, even
% with a coherent source language, the dynamic grudual guarantee is still a
% question. Currently, the dynamic grudual guarantee for our target language \pbc is still an open
% question. According to \citet{yuu2017poly}, the difficulty lies in the definition of term precision
% that preserves the semantics.




% since it is
% unknown whether it holds in \pbc. According to \citet{yuu2017poly} (where they
% have System F$_C$ which is similar to \pbc), the difficulty lies in the definition
% of term precision that preserves the semantics.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% org-ref-default-bibliography: ../paper.bib
%%% End:
