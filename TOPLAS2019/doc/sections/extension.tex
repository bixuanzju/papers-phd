\section{Restoring the Dynamic Gradual Guarantee with Type Parameters}
\label{sec:advanced-extension}

In \cref{sec:type:trans} we have seen an example where a single source expression could
produce two different target expressions with different runtime behaviors. As we
explained, this is due to the guessing nature of the declarative system, and,
from the (source) typing point of view, no guessed type is particularly better than 
any other. As a consequence, this breaks the dynamic gradual guarantee as discussed in \cref{sec:criteria}.

To alleviate this situation, we introduce \textit{static type parameters}, which
are placeholders for monotypes, and \textit{gradual type parameters}, which are
placeholders for monotypes that are consistent with the unknown type. The
concept of static type parameters and gradual type parameters in the context of
gradual typing was first introduced by \citet{garcia2015principal}, and later
played a central role in the work of \citet{yuu2017poly}. In our type system,
type parameters mainly help capture the notion of \textit{representative
  translations}, and should not appear in a source program.
% As far as we know,
% we are the first to employ type parameters in the (implicit) polymorphic
% setting.
With them we are able to recast the dynamic gradual guarantee in terms
of representative translations, and to prove that every well-typed source expression
possesses at least one representative translation. With a
coherence conjecture regarding representative translations, the dynamic gradual
guarantee of our extended source language now can be reduced to that of \pbc,
which, at the time of writing, is still an open question.


% \jeremy{emphasize type parameters are just analysis tool for the purpose of
%   discussing dynamic gradual guarantee, they don't actually appear in program text. }


% The crucial difference
% between them is that the former correspond to existential variables without any
% constraints, while the latter correspond to existential variables with the only
% constraint that they are compared with an unknown type. With static and gradual
% type parameters in place, we are able to reason about dynamic semantics in more
% depth.


\subsection{Declarative Type System}
\label{sec:type-param}

The new syntax of types is given at the top of \cref{fig:exd:type}, with the differences
highlighted. In addition to the types of \cref{fig:decl:subtyping}, we add \emph{static type parameters} $[[static]]$,
and \emph{gradual type parameters} $[[gradual]]$. Both kinds of type parameters are monotypes. The addition of type parameters,
however, leads to two new syntactic categories of types. \emph{Castable types} $[[gc]]$
represent types that can be cast from or to $[[unknown]]$. It includes all
types, except those that contain static type parameters. \emph{Castable monotypes}
$[[tc]]$ are those castable types that are also monotypes.

\begin{figure}[t]
  \centering
  \begin{small}
    \begin{tabular}{lrcl} \toprule
      Types & $[[A]], [[B]]$ & \syndef & $[[int]] \mid [[a]] \mid [[A -> B]] \mid [[\/ a. A]] \mid [[unknown]] \mid \hlmath{[[static]] \mid [[gradual]]} $ \\
      Monotypes & $[[t]], [[s]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[t -> s]] \mid \hlmath{[[static]] \mid [[gradual]]}$ \\
      \hl{Castable Types} & $[[gc]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[gc1 -> gc2]] \mid [[\/ a. gc]] \mid [[unknown]] \mid [[gradual]] $ \\
      \hl{Castable Monotypes} & $[[tc]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[tc1 -> tc2]] \mid [[gradual]]$ \\

      \bottomrule
    \end{tabular}

    \begin{drulepar}[cs]{$ [[dd |- A <~ B ]] $}{Consistent Subtyping}{}
      \drule{tvar}
      \drule{int}
      \drule{arrow}
      \drule{forallR}
      \drule{forallL}
      \hlmath{\drule{unknownLL}} \and
      \hlmath{\drule{unknownRR}} \and
      \hlmath{\drule{spar}} \and
      \hlmath{\drule{gpar}}
    \end{drulepar}
  \end{small}
  \caption{Syntax of types, and consistent subtyping in the extended declarative
  system.}
  \label{fig:exd:type}
\end{figure}


\paragraph{Consistent Subtyping.}
The new definition of consistent subtyping is given at the bottom of
\cref{fig:exd:type}, again with the differences highlighted. Now the unknown type is only a consistent subtype of all
castable types, rather than of all types (\rref{cs-unknownLL}), and vice versa
(\rref{cs-unknownRR}). Moreover, the static type parameter $[[static]]$ is a consistent
subtype of itself (\rref{cs-spar}), and similarly for the gradual type parameter
(\rref{cs-gpar}). From this definition it follows immediately that 
$[[unknown]]$ is incomparable with types that contain static type parameters $[[static]]$,
such as $[[static -> int]]$.

\paragraph{Typing and Translation.}

Given these extensions to types and consistent subtyping, the typing process
remains the same as in \cref{fig:decl-typing}. To account for the
changes in the translation, if we extend \pbc with type parameters as in
\citet{garcia2015principal}, then the translation remains the same as well.

\subsection{Substitutions and Representative Translations}

As we mentioned, type parameters serve as placeholders for monotypes. As a
consequence, wherever a type parameter is used, any \emph{suitable} monotype
could appear just as well. To formalize this observation, we define substitutions for type
parameters as follows:

\begin{definition}[Substitution] Substitutions for type parameters are defined as:
  \begin{enumerate}
    \item Let $\ssubst : [[static]] \to [[t]]$ be a total function mapping static type
      parameters to monotypes. 
    \item Let $\gsubst : [[gradual]] \to [[tc]]$ be a total function mapping gradual type
      parameters to castable monotypes.
    \item Let $\psubst = \gsubst \cup \ssubst$ be a union of $\ssubst$ and $\gsubst$ mapping static and gradual
      type parameters accordingly.
  \end{enumerate}
\end{definition}
\noindent Note that since $[[gradual]]$ might be compared with $[[unknown]]$, only
castable monotypes are suitable substitutes, whereas $[[static]]$
can be replaced by any monotypes. Therefore, we can substitute $[[gradual]]$ for $[[static]]$,
but not the other way around.


Let us go back to our example and its two translations in \cref{sec:type:trans}. The
problem with those translations is that neither $[[int -> int]]$ nor $[[bool ->
int]]$ is general enough. With type parameters, however, we can state a more
\textit{general} translation that covers both through substitution:
\begin{align*}
  f: \forall a. a \to \nat &\byinf (\blam x \unknown {f ~ x})
                          : \unknown \to \nat \\
                          &\trto (\blam x \unknown (\cast {\forall a. a \to \nat} {\gradual \to \nat} f) ~
                          (\hlmath{\cast \unknown \gradual} x))
\end{align*}
The advantage of type parameters is that they help reasoning
about the dynamic semantics. Now we are not limited to a particular choice, such
as $[[int -> int]]$ or $[[bool -> int]]$, which might or might not emit a cast
error at runtime. Instead we have a general choice $[[gradual ->
int]]$. 

What does the more general choice with type parameters tell us? First, we know that in this case, there is no concrete
constraint on $[[a]]$, so we can instantiate it with a type parameter. Second,
the fact that the general choice uses $[[gradual]]$ rather than
$[[static]]$ indicates that any chosen instantiation needs to be a castable type.
It follows that any concrete instantiation will have an impact on the
runtime behavior; therefore it is best to instantiate $[[a]]$ with
$[[unknown]]$. However, type inference cannot instantiate $[[a]]$ with
$[[unknown]]$, and substitution cannot replace $[[gradual]]$ with $[[unknown]]$ either.
This means that we need a syntactic refinement process of the translated programs in
order to replace type parameters with allowed gradual types.

\paragraph{Syntactic Refinement.}

We define syntactic refinement of the translated expressions as follows. As
$[[static]]$ denotes no constraints at all, substituting it with any monotype
would work. Here we arbitrarily use $[[int]]$. We interpret
$[[gradual]]$ as $[[unknown]]$ since any monotype could possibly lead to a cast
error.

\begin{definition}[Syntactic Refinement] The syntactic refinement of a
  translated expression $[[pe]]$ is denoted by $\erasetp s$, and defined as follows:
  \begin{center}
\begin{tabular}{lllllll} \toprule
  $\erasetp{\nat}$ &$=$ & $ \nat$ &  &   $\erasetp{a} $ & $ = $ & $a $ \\
  $\erasetp{A \to B}$ &$=$ & $ \erasetp{A} \to \erasetp{B}$ &  &   $\erasetp {\forall a. A} $ & $ = $ & $ \forall a . \erasetp{A} $ \\
  $\erasetp{\unknown}$ &$=$ & $\unknown$ &  &   $\erasetp {\static} $ & $ = $ & $\nat$ \\
  $\erasetp{\gradual}$ &$=$ & $\unknown$ &  \\ \bottomrule
\end{tabular}

  \end{center}
\end{definition}
% \bruno{Can we align the ``='' and the types?}
\noindent Applying the syntactic refinement to the translated
expression, we get
  \[
    (\blam x \unknown (\cast {\forall a. a \to \nat} { \hlmath[blue!40]{\unknown} \to \nat} f) ~
    (\cast \unknown {\hlmath[blue!40]{\unknown}} x))
  \]
where two $[[gradual]]$ are refined by $[[unknown]]$ as highlighted.
It is easy to verify that both applying this expression to $3$ and to
$\mathit{true}$ now results in a translation that evaluates to
a value.

\paragraph{Representative Translations.}
To decide whether one translation is more general than the other, we define a preorder
between translations.

\begin{definition}[Translation Pre-order]
  Suppose $[[dd |- e : A ~~> pe1]]$ and $[[dd |- e : A ~~> pe2]]$,
  we define $[[pe1]] \leq [[pe2]]$ to mean $[[pe2]] \aeq [[S(pe1)]]$ for
  some $[[S]]$.
\end{definition}

\begin{restatable}[]{proposition}{propparalpha}
  \label{prop:parameter:alpha}
  If $[[ pe1 ]] \leq [[pe2]]$ and $[[ pe2 ]] \leq [[pe1]]$, then $[[pe1]]$ and
  $[[pe2]]$ are $\alpha$-equivalent (i.e., equivalent up to renaming of type parameters).
\end{restatable}

The preorder between translations gives rise to a notion of
what we call \textit{representative translations}:

\begin{definition}[Representative Translation]
  A translation $[[pe]]$ is said to be a representative translation of a typing
  derivation $[[dd |- e : A ~~> pe]]$ if and only if for any other translation
  $[[dd |- e : A ~~> pe']]$ such that $[[pe']] \leq [[pe]]$, we have $[[pe]]
  \leq [[pe']]$. From now on we use $[[rpe]]$ to denote a representative
  translation.
\end{definition}

An important property of representative translations, which we conjecture for
the lack of rigorous proof, is that if there exists any translation of an
expression that (after syntactic refinement) can reduce to a value, so can a
representative translation of that expression. Conversely, if a
representative translation runs into a blame, then no translation of that
expression can reduce to a value.

\begin{conjecture}[Property of Representative Translations]\label{lemma:repr}
  For any expression $[[e]]$ such that $[[ dd |- e : A ~~> pe ]]$ and $[[ dd |- e : A ~~> rpe ]]$ and
  $\forall [[CC]].\, [[CC : (dd |- A) ~~> (empty |- int) ]]   $, we have
  \begin{itemize}
  \item If $  [[CC]] \{  \erasetp{[[pe]]} \}  [[==>]] [[n]]$, then $ [[CC]] \{   \erasetp{[[rpe]]}   \} [[==>]] [[n]]$.
  \item If $[[CC]] \{ \erasetp {[[rpe]]}   \} [[==>]] [[blame]]$, then $ [[CC]] \{ \erasetp {[[pe]]} \}  [[==>]] [[blame]]$.
  \end{itemize}
\end{conjecture}

Given this conjecture, we can state a stricter coherence property (without the
``up to casts'' part) between any two representative translations. We first
strengthen \cref{conj:coher} following \citet{amal2017blame}:

\begin{definition}[Contextual Approximation \`a la \citet{amal2017blame}] \leavevmode
  \begin{center}
  \begin{tabular}{lll}
$[[dd]] \vdash \ctxappro{[[pe1]]}{[[pe2]]}{[[A]]}$ & $\defeq$ & $[[ dd |- pe1 : A  ]] \land [[dd |- pe2 : A ]] \ \land $ \\
                                                   & & for all $\mathcal{C}.\, [[ CC : (dd |- A) ~~> (empty |- int) ]] \Longrightarrow$ \\
                                                   & &  $\quad (\mathcal{C}\{ \erasetp{[[pe1]]} \}   \Downarrow [[n]] \Longrightarrow  \mathcal{C} \{ \erasetp{[[ pe2 ]]}  \}  \reduce [[n]]) \ \land$ \\
                                                   & & $\quad (\mathcal{C} \{ \erasetp{[[ pe1 ]]} \} \reduce \blamev \Longrightarrow \mathcal{C} \{ \erasetp{[[ pe2 ]]}  \}  \reduce \blamev)$

  \end{tabular}
  \end{center}
\end{definition}
The only difference is
that now when a program containing $[[pe1]]$ reduces to a value, so does one
containing $[[pe2]]$.


From \cref{lemma:repr}, it follows that coherence holds between
two representative translations of the same expression.

\begin{corollary}[Coherence for Representative Translations]
  For any expression $[[e]]$
  such that $[[ dd |- e : A ~~> rpe1    ]]$ and $[[ dd |- e : A ~~> rpe2    ]]$, we have
  $[[ dd ]] \vdash \ctxeq{[[rpe1]]}{[[rpe2]]}{[[A]]} $.
\end{corollary}

We have proved that for every typing derivation, at least one representative translation exists.

\begin{restatable}[Representative Translation for Typing]{lemma}{lemmareptyping}
  \label{lemma:rep:typing}
  For any typing derivation $[[dd |- e : A]]$ there exists at least one
  representative translation $r$ such that $[[dd |- e : A ~~> rpe]]$.
\end{restatable}

For our example, $(\blam x \unknown (\cast {\forall a. a \to \nat} {\gradual \to
  \nat} f) ~ (\cast \unknown \gradual x))$ is a representative translation,
while the other two are not.


\subsection{Dynamic Gradual Guarantee, Reloaded}

Given the above propositions, we are ready to revisit the dynamic gradual
guarantee. The nice thing about representative translations is that the
dynamic gradual guarantee of our source language is essentially that of \pbc,
our target language. However, the dynamic gradual guarantee for \pbc is still an
open question. According to \citet{yuu2017poly}, the difficulty lies in the
definition of term precision that preserves the semantics. We leave it here as a
conjecture as well. From a declarative point of view, we cannot prevent the
system from picking undesirable instantiations, but we know that some choices
are better than the others, so we can restrict the discussion of dynamic gradual
guarantee to representative translations.

\begin{conjecture}[Dynamic Gradual Guarantee in terms of Representative Translations]
  Suppose $e' \lessp e$,
  \begin{enumerate}
  \item If $[[empty |- e : A ~~> rpe]]$, $\erasetp {r} \Downarrow v$,
    then for some $B$ and $r'$, we have $[[ empty |- e' : B ~~> rpe']]$,
    and $B \lessp A$,
    and $\erasetp {r'} \Downarrow v'$,
    and $v' \lessp v$.
  \item If $[[empty |- e' : B ~~> rpe']]$, $\erasetp {r'} \Downarrow v'$,
    then for some $A$ and $[[rpe]]$, we have $ [[empty |- e : A ~~> rpe]]$,
    and $B \lessp A$. Moreover,
    $\erasetp r \Downarrow v$ and $v' \lessp v$,
    or $\erasetp r \Downarrow \blamev$.
  \end{enumerate}
\end{conjecture}

For the example in \cref{sec:criteria}, now we know that the representative
translation of the right one will evaluate to $1$ as well.
\begin{mathpar}
  (\blam{f}{\forall a. a \to \nat}{\blam{x}{\nat}{f~x}})~(\lambda x .\, 1)~3 \and
  (\blam{f}{\forall a. a \to \nat}{\blam{x}{\unknown}{f~x}})~(\lambda x .\, 1)~3
\end{mathpar}

More importantly, in what follows, we show that our extended algorithm is able to find those representative translations.


\subsection{Extended Algorithmic Type System}
\label{subsec:exd-algo}

\begin{figure}[t]
  \centering
  \begin{small}
    \begin{tabular}{lrcl} \toprule
      Types & $[[aA]], [[aB]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[evar]] \mid [[aA -> aB]] \mid [[\/ a. aA]] \mid [[unknown]] \mid \hlmath{[[static]] \mid [[gradual]]} $ \\
      Monotypes & $[[at]], [[as]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[evar]] \mid [[at -> as]] \mid \hlmath{[[static]] \mid [[gradual]]}$ \\
      \hl{Existential variables} & $[[evar]]$ & \syndef & $[[sa]]  \mid [[ga]]  $   \\
      \hl{Castable Types} & $[[agc]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[evar]] \mid [[agc1 -> agc2]] \mid [[\/ a. agc]] \mid [[unknown]] \mid [[gradual]] $ \\
      \hl{Castable Monotypes} & $[[atc]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[evar]] \mid [[atc1 -> atc2]] \mid [[gradual]]$ \\
      Algorithmic Contexts & $[[GG]], [[DD]], [[TT]]$ & \syndef & $[[empty]] \mid [[GG , x : aA]] \mid [[GG , a]] \mid [[GG , evar]]  \mid \hlmath{[[GG, sa = at]] \mid [[GG, ga = atc]]} \mid [[ GG, mevar ]] $ \\
      Complete Contexts & $[[OO]]$ & \syndef & $[[empty]] \mid [[OO , x : aA]] \mid [[OO , a]] \mid \hlmath{[[OO, sa = at]] \mid [[OO, ga = atc]]} \mid [[OO, mevar]] $ \\
      \bottomrule
    \end{tabular}
  \end{small}
  \caption{Syntax of types, contexts and consistent subtyping in the extended algorithmic system.}
  \label{fig:exd:algo:type}
\end{figure}


% \jeremy{the example is wrong, we need a new example to motivate}
To understand the design choices involved in the new algorithmic system, we
consider the following algorithmic typing example:
\[
  f: [[ \/a .  a -> int  ]], x : [[unknown]] \byinf [[ f x ]] \infto [[int]]  \dashv f : [[\/a . a -> int]], x : [[unknown]], \genA
\]
Compared with the declarative typing, where we have many choices (e.g., $[[int -> int]]$, $[[bool -> int]]$, and so on)
to instantiate $[[\/ a. a -> int]]$, the algorithm computes
the instantiation $[[ evar -> int ]]$ with $[[evar]]$ unsolved in the output context.
What can we know from the algorithmic typing? First we know that, here $[[evar]]$
is \textit{not constrained} by the typing problem. Second, and more importantly,
$[[evar]]$ has been compared with an unknown type (when typing $([[ f x ]])$).
Therefore, it is possible to make a more refined distinction
between different kinds of existential variables. The first
kind of existential variables are those that indeed have no constraints at all,
as they do not affect the dynamic semantics; while the second kind (as in this example) are
those where the only constraint is that
\textit{the variable was once compared with an unknown type}~\citep{garcia2015principal}.

The syntax of types is shown in \cref{fig:exd:algo:type}. A notable
difference, apart from the addition of static and gradual parameters, is that we
further split existential variables $[[evar]]$ into static existential variables
$[[ sa ]]$ and gradual existential variables $[[ga]]$.
Depending on whether an existential variable has been
compared with $[[unknown]]$ or not, its solution space changes. More
specifically, static existential variables can be solved to a monotype
$[[at]]$, whereas gradual existential variables can only be solved to a
castable monotype $[[atc]]$, as can be seen in the changes of algorithmic
contexts and complete contexts. As a result, the typing result for the above example
now becomes
\[
  f: [[ \/a .  a -> int  ]], x : [[unknown]] \byinf [[ f x ]] \infto [[int]]  \dashv f : [[\/a . a -> int]], x : [[unknown]], \hlmath{[[ga]]}
\]
since we can solve any unconstrained $[[ga]]$ to $[[gradual]]$, it is easy to
verify that the resulting translation is indeed a representative translation.

Our extended algorithm is novel in the following aspects. We naturally extend
the concept of existential variables~\citep{dunfield2013complete} to deal with
comparisons between existential variables and unknown types. Unlike
\citet{garcia2015principal}, where they use an extra set to store types that
have been compared with unknown types, our two kinds of existential variables emphasize
the type distinction better, and correspond more closely to the two kinds of type parameters,
as we can solve $[[sa]]$ to $ [[static]]$ and $[[ga]] $ to $ [[gradual]]$.

The implementation of the algorithm can be found in the supplementary materials.


\paragraph{Extended Algorithmic Consistent Subtyping}


\begin{figure}[t]
  \centering
  \begin{small}
   \begin{drulepar}[as]{$ [[GG |- aA <~ aB -| DD ]] $}{Algorithmic Consistent Subtyping}
     \drule{tvar}
     \drule{int}
     \drule{evar} \and
     \hlmath{\drule{spar}} \and
     \hlmath{\drule{gpar}} \and
     \hlmath{\drule{unknownLL}}
     \hlmath{\drule{unknownRR}} \and
     \drule{arrow}
     \drule{forallR} \and
     \hlmath{\drule{forallLL}} \and
     \drule{instL}
     \drule{instR}
   \end{drulepar}
  \end{small}
  \caption{Extended algorithmic consistent subtyping}
  \label{fig:exd:algo:sub}
\end{figure}

While the changes in the syntax seem negligible, the addition of static and
gradual type parameters changes the algorithmic judgments in a significant way.
We first discuss the algorithmic consistent subtyping, which is shown in \cref{fig:exd:algo:sub}.
For notational convenience, when static and
gradual existential variables have the same rule form, we compress them into one rule. For
example, \rref{as-evar} is really two rules $[[ GG[sa] |- sa <~ sa -| GG[sa] ]]$
and $[[ GG[ga] |- ga <~ ga -| GG[ga] ]]$; same for \rref{as-instL,as-instR}.

\Rref{as-spar,as-gpar} are direct analogies of \rref{cs-spar,cs-gpar}. Though
looking simple, \rref{as-unknownLL,as-unknownRR} deserve much explanation. To
understand what the output context $[[ [agc]GG ]]$ is for, let us first see why
this seemingly intuitive rule $[[ GG |- unknown <~ agc -| GG ]]$ (like
\rref{as-unknownL} in the original algorithmic system) is wrong. Consider the
judgment $[[ sa |- unknown <~ sa -> sa -| sa ]]$, which seems fine. If this
holds, then -- since $[[sa]]$ is unsolved in the output context -- we can solve
it to $[[ static ]]$ for example (recall that $[[sa]]$ can be solved to some
monotype), resulting in $[[ unknown <~ static -> static ]]$. However, this is in
direct conflict with \rref{cs-unknownLL} in the declarative system precisely
because $[[ static -> static ]]$ is not a castable type! A possible solution
would be to transform all static existential variables to gradual existential
variables within $[[agc]]$ whenever it is being compared to $[[ unknown ]]$:
while $[[ sa |- unknown <~ sa -> sa -| sa ]]$ does not hold, $[[ ga |- unknown
<~ ga -> ga -| ga ]]$ does. While substituting static existential variables with
gradual existential variables seems to be intuitively correct, it is rather hard
to formulate---not only do we need to perform substitution in $[[agc]]$, we also
need to substitute accordingly in both the input and output contexts in order to
ensure that no existential variables become unbound. However, making such changes is
at odds with the interpretation of input contexts: they are ``input'', which
evolve into output contexts with more variables solved. Therefore, in line with
the use of input contexts, a simple solution is to generate a
new gradual existential variable and solve the static existential variable to it
in the output context, without touching $[[agc]]$ at all. So we have $[[ sa |- unknown <~ sa -> sa -| ga, sa = ga ]]$.

Based on the above discussion, the following defines $[[ [aA]GG ]]$:
\begin{definition}$[[ [aA]GG ]]$ is defined inductively as follows  \label{def:contamination} %
  \begin{center}
    \begin{tabular}{llll} \toprule
     $[[ [aA] empty    ]]$ & = &  $[[empty]]$  & \\
    $[[ [aA] (GG, x : aA)  ]]$ &=& $[[ [aA] GG , x : aA     ]]$ & \\
    $[[ [aA] (GG, a)  ]]$ &=& $[[ [aA] GG , a     ]]$ & \\
    $[[ [aA] (GG, sa)  ]]$ &=& $[[ [aA] GG , ga , sa = ga  ]]$  & if $[[sa]]$ occurs in $[[aA]]$     \\
    $[[ [aA] (GG, sa)  ]]$ &=& $[[ [aA] GG , sa     ]]$     & if $[[sa]]$ does not occur in $[[aA]]$  \\
    $[[ [aA] (GG, ga)  ]]$ &=& $[[ [aA] GG , ga     ]]$ & \\
    $[[ [aA] (GG, evar = at)  ]]$ &=& $[[ [aA] GG , evar = at     ]]$ & \\
    $[[ [aA] (GG, mevar)  ]]$ &=& $[[ [aA] GG , mevar     ]]$ & \\ \bottomrule
    \end{tabular}
  \end{center}
\end{definition}
\noindent $[[ [aA]GG ]]$ solves all static existential variables found within $[[aA]]$ to fresh
gradual existential variables in $[[GG]]$. Notice the case for $[[ [aA] (GG, sa)]]$
is exactly what we have just described.

\Rref{as-forallLL} is slightly different from \rref{as-forallL} in the original
algorithmic system in that we replace $[[a]]$ with a new static existential
variable $[[sa]]$. Note that $[[sa]]$ might be solved to a gradual existential
variable later. The rest of the rules are the same as those in the original system.


\begin{figure}[t]
  \centering
  \begin{small}

   \begin{drulepar}[instl]{$ [[ GG |- evar <~~ aA -| DD   ]] $}{Instantiation I}
     \hlmath{\drule{solveS}} \and
     \hlmath{\drule{solveG}} \and
     \hlmath{\drule{solveUS}} \and
     \hlmath{\drule{solveUG}} \and
     \hlmath{\drule{reachSGOne}} \and
     \hlmath{\drule{reachSGTwo}} \and
     \hlmath{\drule{reachOther}} \and
     \drule{forallR}
     \drule{arr}
   \end{drulepar}

   \begin{drulepar}[instr]{$ [[ GG |- aA <~~ evar -| DD   ]] $}{Instantiation II, excerpt}
     \hlmath{\drule{forallLL}} \and
   \end{drulepar}
  \end{small}

  \caption{Instantiation in the extended algorithmic system}
  \label{fig:exd:inst}

\end{figure}

\paragraph{Extended Instantiation}

The instantiation judgments shown in \cref{fig:exd:inst} also change
significantly. The complication comes from the fact that now we have two different
kinds of existential variables, and the relative order they appear in the
context affects their solutions.


\Rref{instl-solveS, instl-solveG} are the refinement to \rref{instl-solve} in
the original system. The next two rules deal with situations where one side is
an existential variable and the other side is an unknown type.
\Rref{instl-solveUS} is a special case of \rref{as-unknownRR} where we create a
new gradual existential variable $[[ga]]$ and set the solution of $[[sa]]$ to be
$[[ga]]$ in the output context. \Rref{instl-solveUG} is the same as
\rref{instl-solveU} in the original system and simply propagates the input
context. The next two rules \rref*{instl-reachSG1,instl-reachSG2} are a bit
involved, but they both answer to the same question: how to solve a gradual
existential variable when it is declared after some static existential variable.
More concretely, in \rref{instl-reachSG1}, we feel that we need to solve
$[[gb]]$ to another existential variable. However, simply setting $[[ gb = sa]]$ and leaving $[[sa]]$ untouched
in the output context is wrong. The reason is that $[[gb]]$ could be a gradual existential
variable created by \rref{as-unknownLL}/\rref*{as-unknownRR} and solving $[[gb]]$ to a static existential
variable would result in the same problem as we have discussed. Instead, we create another new gradual
existential variable $[[ga]]$ and set the solutions of both $[[sa]]$ and $[[gb]]$ to it; similarly in \rref{instl-reachSG2}.
\Rref{instl-reachOther} deals with the other cases (e.g., $[[ sa <~~ sb  ]]$, $[[ ga <~~ gb  ]]$ and so on).
In those cases, we employ the same strategy as in the original system.

As for the other instantiation judgment, most of the rules are symmetric and thus omitted.
The only interesting rule is \rref*{instr-forallLL}, which is similar to what we did for \rref{as-forallLL}.



\paragraph{Algorithmic Typing and Metatheory}

Fortunately, the changes in the algorithmic bidirectional system are minimal: we replace
every existential variable with a static existential variable.
Furthermore, we proved that the extended
algorithmic system is sound and complete with respect to the extended
declarative system. The proofs can be found in the appendix.



\paragraph{Do We Really Need Type Parameters in the Algorithmic System?}

As we mentioned earlier, type parameters in the declarative system are merely an
analysis tool, and in practice, type parameters are inaccessible to
programmers. For the sake of proving soundness and completeness, we have to
endow the algorithmic system with type parameters. However, the algorithmic
system already has static and gradual existential variables, which can serve the same
purpose. In that regard, we could directly solve every \textit{unsolved} static and
gradual existential variable in the output context to $[[int]]$ and
$[[unknown]]$, respectively.


% \jeremy{example of showing finding the representative translation?}
% \ningning{Include a simple discussion?: since type parameters are used to help
%   with reasoning, in practice, programmers are actually not allowed to write
%   them. Therefore, the algorithm could directly set unsolved static existential
%   to integers and gradual existential to unknowns after type checking as
%   algorithmic refinement process, without even knowing type parameters. }

% \subsection{Discussion}

\subsection{Restricted Generalization}

In \cref{sec:type:trans}, we discussed the issue that the translation produces
multiple target expressions due to the different choices for instantiations, and
those translations have different dynamic semantics. Besides that, there is
another cause for multiple translations: redundant generalization during
translation by \rref{gen}. Consider the simple expression $[[(\x:int. x) 1]]$,
the following shows two possible translations:
\begin{align*}
  [[empty |- (\x : int . x) 1 : int ]] &[[~~>]] [[ (\x : int . x) (<int `-> int> 1)]]
  \\
  [[empty |- (\x : int . x) 1 : int ]] &[[~~>]]  [[ (\x : int . x) (<\/ a. int `-> int> (/\ a. 1))]]
\end{align*}
The difference comes from the fact that in the second translation, we apply
\rref{gen} while typing $1$ to get $[[empty |- 1 : \/ a. int]]$. As a consequence, the translation of $1$
is accompanied by a cast from $[[\/ a. int]]$ to $[[int]]$ since the former is a
consistent subtype of the latter. This difference is harmless, because obviously
these two expressions will reduce to the same value in \pbc, thus preserving
coherence (up to cast error). While it is not going to break coherence,
it does result in multiple representative translations for one
expression (e.g., the above two translations are both the representative translations).

There are several ways to make the translation process more deterministic. For
example, we can restrict generalization to happen only in let expressions and
require let expressions to include annotations, as $[[ let x : A = e1 in e2 ]]$.
Another feasible option would be to give a declarative, bidirectional system as
the specification (instead of the type assignment one), in the same spirit of
\citet{dunfield2013complete}. Then we can restrict generalization to be
performed through annotations in checking mode.

With restricted generalization, we hypothesize that now each expression has exactly
one representative translation (up to renaming of fresh type parameters).
Instead of calling it a \textit{representative} translation, we can say it is a
\textit{principal} translation. Of course the above is only a sketch; we have
not defined the corresponding rules, nor studied metatheory.


\begin{comment}
\subsubsection{Interpretation of Type Parameters}
\label{subsec:type-par}

% \jeremy{If I understand it correctly, we actually used these two interpretations
%   in the extended declarative system. Def 8.1 (substitutions) is the first
%   interpretation; and Def 8.2 (syntactic refinement) is the second
%   interpretation in that $[[static]]$ is irrelevant to program execution so we
%   can replace it with any type, whereas $[[gradual]]$ is relevant so we replace
%   it with unknown }

In \cref{sec:type-param}, we introduced type parameters into our type system. It turns
out that type parameters are a useful tool to help us identify
representative translations and reason about the dynamic semantics of the
type system. But what are type parameters, exactly? Below we provide two plausible
interpretations.

The first interpretation of type parameters (the one we adopted) is that they are placeholders for
monotypes. This is to say, their meaning is given by substitution, and replacing
type parameters with other monotypes should not break typing:

\begin{proposition}
  If $[[dd |- e : A]]$, then $\psubst ([[dd]]) \vdash \psubst ([[e]]) : \psubst ([[A]])$.
\end{proposition}

\jeremy{See Proposition 1 of Principle scheme for gradual programs, where they also have exactly the same proposition, but they call it
type polymorphism! how this compare to the second interpretation?}

In practice, we should not allow programmers to write type parameters explicitly
in a program, as type parameters are only generated during typing process, and
get refined before evaluation. As a result, we can hide all the details of type
parameters from programmers and internally replace them with suitable concrete
types when necessary. This also reflects the point we discussed in the end of
\cref{subsec:exd-algo}.

On the other hand, we can interpret type parameters using \textit{polymorphism}.
In this sense, both of them can be extracted to generate type abstractions.
However, there is one subtle difference. That is, static type parameter
indicates \textit{parametric polymorphism} in the traditional sense, which is
irrelevant to program execution; while gradual type parameter indicates
\textit{gradual polymorphism}, which means it has no typing constraints but is
relevant to program execution \citep{garcia2015principal}. This interpretation
suggests that we might need a more refined distinction between type
abstractions, such as \citet{yuu2017poly}.

We argue that the extension of type parameters is \textit{a} feasible way to
reason about the dynamic semantics in a implicit polymorphic language, but it is
not necessarily \textit{the} only way. Still, it remains to see if
our discussion sheds lights on the study of dynamic semantics for
gradual languages with implicit polymorphism.

\end{comment}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../paper"
%%% org-ref-default-bibliography: ../paper.bib
%%% End:
